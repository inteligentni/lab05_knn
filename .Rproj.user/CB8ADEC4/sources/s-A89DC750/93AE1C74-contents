---
title: "K Nearest Neighbours (KNN)"
output:
  pdf_document: default
urlcolor: blue
---

### Examine the data set

We will use the same data set (Carseats) as in the previous Lab. 
```{r}
library(ISLR)

str(Carseats)
```

As we did before, we will introduce a categorical (factor) variable - HighSales - to be used as the outcome variable (variable defining the class for each observation). 
If a sale is greater than the 3rd quartile (9.32), it qualified as high sale:
```{r}
Carseats$HighSales <- ifelse(test = Carseats$Sales > 9.32, yes = 'Yes', no = 'No')
Carseats$HighSales <- as.factor(Carseats$HighSales)
```

We'll remove the Sales variable, as we do not need it any more
```{r}
Carseats <- Carseats[,-1]
```

Recall that kNN algorithm primarily works with numerical data. So, if we want to use categorical and/or binary variables, we have to transform them into numerical variables. 

Recall also that kNN is very sensitive to differences in the value range of predictor variables. This is because predictors with wider range of values (e.g. Price) would diminish the influence of variables with significantly narrower range (e.g. Education). 

Let's check our variables and their value ranges
```{r}
summary(Carseats)
```

We should, obviously, rescale our numerical variables. Rescaling can be generally done in two ways:

* *normalization* - reducing variable values to a common value range, typically [0,1]; this is often done using the formula: (X - min(X))/(max(X) - min(X))
* *standardization* - rescaling variables so that their mean=0 and SD=1; for variable X that is normally distributed, this is done by computing: (X - mean(X))/SD(X); if variable X is not normally distributed, then, standardization is typically done using median and inter-quartile range (IQR): (X - median(X))/IQR(X)  

Normalization should be avoided if (numerical) variables have outliers; standardization should be used instead. In the absence of outliers, either of the two can be used.

Check the presence of outliers:
```{r}
numeric.vars <- c(1:5,7,8) # indices of numeric columns
apply(X = Carseats[,numeric.vars], # select numeric columns
      MARGIN = 2, # apply the function to columns
      FUN = function(x) length(boxplot.stats(x)$out)) # the function to be applied to each column
```
Only 2 variables (CompPrice, Price) have just a few outliers; hence, either of the scaling methods can be used.


### Standardize numerical attributes

We will rescale our numerical variables by standardizing them (typical approach). To determine how to standardize the variables, we need to check their distribution (if they follow Normal distribution or not). 

We will use *Shapiro-Wilk test* to check for normality. The null hypothesis of this test is that a sample comes from a normally distributed population; if the test is not significant (p>0.05), we can assume that the null hypothesis holds.
```{r}
# apply the test to each numerical column (variable)
apply(X = Carseats[,numeric.vars], MARGIN = 2, FUN = shapiro.test)
```
Only CompPrice and Price are normally distributed. So, we will standardize Price and CompPrice using mean and SD, and for other variables, we'll use median and IQR. 

To do the scaling, we will use the *scale* f. (from the base package):
```{r}
?scale
```

We'll start by rescaling variable that are not normally distributed
```{r}
# select not-normally distributed numerical columns (variables)
carseats.st <- Carseats[, c(2,3,4,7,8)]
# apply the scalling function to each column
carseats.st <- as.data.frame(apply(X = carseats.st, 
                                   MARGIN = 2, 
                                   FUN = function(x) scale(x, center = median(x), 
                                                           scale = IQR(x))))
# note: apply() f. returns a list; as we need a data frame, we use the as.data.frame() f. to transform the output of apply() into a data frame
```

Then, we'll standardize and add normally distributed ones
```{r}
carseats.st$Price <- as.vector(scale(x = Carseats$Price, center = TRUE, scale = TRUE))
carseats.st$CompPrice <- as.vector(scale(x = Carseats$CompPrice, center = TRUE, scale = TRUE))
```
Note: the scale() f. returns a matrix with just one column; so, it is effectively a vector
and we transform it into a vector using the as.vector() f.

Now, we need to handle binary and categorical variables.

### Transform factor (binary and categorical) variables

Transform binary variables into numerical
```{r}
carseats.st$Urban <- as.integer(Carseats$Urban)
carseats.st$US <- as.integer(Carseats$US)
```

It is often considered more correct to first encode categorical variables as binary dummy variables, and then transform the resulting binary variables into numerical ones. 
However, for simplicity reasons, and since our categorical variable - ShelveLoc - is ordered, we will directly transform it into a numerical variable. 

First, let's check the order of ShelveLoc levels
```{r}
levels(Carseats$ShelveLoc)
```

Obviously, the order is not a 'natural' one. So, we need to change the order of levels:
```{r}
Carseats$ShelveLoc <- factor(Carseats$ShelveLoc, levels = c("Bad", "Medium", "Good"))
levels(Carseats$ShelveLoc)
```

Now, we can transform ShelveLoc into numerical variable
```{r}
carseats.st$ShelveLoc <- as.numeric(Carseats$ShelveLoc)
```

**TASK**: try to create dummy variables for ShelveLoc and build a model with these new variables; the following page shows how to create dummy variables using the *caret* package: http://topepo.github.io/caret/pre-processing.html#dummy

Finaly, add the outcome (class) variable
```{r}
carseats.st$HighSales <- Carseats$HighSales
```

Examine the transformed data set:
```{r}
str(carseats.st)
```

```{r}
summary(carseats.st)
```


Now that we have prepared the data, we can proceed to create sets for training and testing.

### Create train and test data sets

We'll do this as we did it in the previous Labs
```{r}
library(caret)
```

We'll take 80% of observations for the training set and the rest for the test set
```{r}
set.seed(1010)
train.indices <- createDataPartition(carseats.st$HighSales, p = 0.8, list = FALSE)
train.data <- carseats.st[train.indices,]
test.data <- carseats.st[-train.indices,]
```

### Model building

To build a kNN classification model, we will use the *knn* f. from the *class* package
```{r}
library(class)

?knn
```
As the knn() function description indicates, we need to provide the function with:

* training data without the class variable
* test data without the class variable 
* class values for the the training set
* number of neighbours to consider

```{r}
knn.pred <- knn(train = train.data[,-11], 
                  test = test.data[,-11],
                  cl = train.data$HighSales,
                  k = 5) # 5 is chosen here just as a random guess
```

The result of the knn f. are, in fact, predictions on the test set
```{r}
head(knn.pred)
```

To evaluate the results, we'll first create the confusion matrix:
```{r}
knn.cm <- table(true = test.data$HighSales, predicted = knn.pred)
knn.cm
```

We'll use the function for computing the evaluation metrics, from the previous Labs
```{r}
compute.eval.metrics <- function(cmatrix) {
  TP <- cmatrix[1,1] # true positive
  TN <- cmatrix[2,2] # true negative
  FP <- cmatrix[2,1] # false positive
  FN <- cmatrix[1,2] # false negative
  acc = sum(diag(cmatrix)) / sum(cmatrix)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2*precision*recall / (precision + recall)
  c(accuracy = acc, precision = precision, recall = recall, F1 = F1)
}
```

Compute the evaluation metrics based on the confusion matrix
```{r}
knn.eval <- compute.eval.metrics(knn.cm)
knn.eval
```

Not bad, but we might be able to do better by choosing another value for k.

We made a guess about the number of neighbours, and might not have made the best guess. Intead of guessing, we'll cross-validate models with several different values for k, and see which one gives the best performance; then, we'll use the test set to evaluate the model that proves to be the best on cross-validation.

As we did with decision trees, we'll use the *caret* package to find the optimal parameter value through cross validation
First, define cross-validation (cv) parameters; we'll do 10-fold cross-validation
```{r}
numFolds = trainControl( method = "cv", number = 10 )
```

Then, define the range of *k* values to examine in the cross-validation; we'll take odd numbers between 3 and 25 - recall that in case of binary classification, it is recommended to choose an odd number for k 
```{r}
cpGrid = expand.grid(.k = seq(from=3, to = 25, by = 2)) 
```

Now, train the model through cross-validation
```{r}
set.seed(1010)
knn.cv <- train(HighSales ~ ., data = train.data, method = "knn", 
                trControl = numFolds, tuneGrid = cpGrid)
```

Let's examine the obtained results:
```{r}
knn.cv
```

We can get a better insight into the cross-validation results by plotting them
```{r}
plot(knn.cv)
```

k=9 proved to be the best value. Let's build a model with that value for k:
```{r}
knn.pred2 <- knn(train = train.data[,-11],
                  test = test.data[,-11],
                  cl = train.data$HighSales,
                  k = 9)
```

Create the confusion matrix
```{r}
knn.cm2 <- table(true = test.data$HighSales, predicted = knn.pred2)
knn.cm2
```

Compute evaluation measures
```{r}
knn.eval2 <- compute.eval.metrics(knn.cm2)
knn.eval2
```

This model seems to be better than the previous one, but let's compare the metrics of the two models to check how the new model fares
```{r}
data.frame(rbind(knn.eval, knn.eval2), row.names = c("knn 1", "knn 2"))
```

The first model (knn1) is better in term of precision, but weaker with respect to recall.

**TASK** Create a new model by taking only a subset of variables, for example, those that proved relevant in the DT model, and compare the performance with the previously built models.


Potentially useful articles:

* https://rpubs.com/njvijay/16444
* http://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/
