---
title: "Decision trees"
output:
  pdf_document: default
urlcolor: blue
---

# Load and prepare data set

For this lab, we will use the *Carseats* data set from the *ISLR* package.

(install and) load the package with the data set
```{r message=FALSE}
# load ISLR package
# install.packages('ISLR')
library(ISLR)
```

Carseats is a simulated data set containing sales of child car seats at 400 different stores. To inform about this data set, type *?Carseats*.

```{r}
# get the Carseats dataset docs
?Carseats
```

We'll start by examining the structure of the data set.

```{r}
# print dataset structure
str(Carseats)
```

Based on the *Sales* variable, we'll add a new categorical (factor) variable to be used for classification.

```{r}
# print Sales variable distribution
summary(Carseats$Sales)
```

Name the new variable *HighSales* and define it as a factor with two values: 'Yes' if *Sales* value is greater than the 3rd quartile (9.32), and 'No' otherwise.

```{r}
# get the 3rd quartile of the Sales variable
sales.3Q <- quantile(Carseats$Sales, 0.75)

# create a new variable HighSales based on the value of the Sales variable
Carseats$HighSales <- ifelse(test = Carseats$Sales > sales.3Q, yes = 'Yes', no = 'No')
head(Carseats)

# class of the HighSales variable
class(Carseats$HighSales)
```

We have created a character vector. Now, we need to transform it into a factor variable.

```{r}
# convert HighSales into a factor variable
Carseats$HighSales <- as.factor(Carseats$HighSales)
head(Carseats$HighSales)
```

Let's check the distribution of the two values.

```{r}
# get the distribution of the HighSales variable
table(Carseats$HighSales)

# get the proportions of the HighSales variable
prop.table(table(Carseats$HighSales))
```

So, in 75.25% of shops, the company have not achieved high sales.

The objective is to develop a model that would be able to predict if the company will have a large sale in a certain shop. More precisely, the company is interested in spotting shops where high sales are not expected, so that it can take some interventions to improve sales. This means that the class we are particularly interested in (the so-called 'positive class') is *No*.

# Create train and test datasets

Remove the *Sales* variable as we do not need it anymore.

```{r}
# remove Sales variable
Carseats$Sales <- NULL
```

We should randomly select observations for training and testing. We should also assure that the distribution of the output variable (*HighSales*) is the same in both datasets (train and test); this is referred to as *stratified partitioning*. To do that easily, we'll use appropriate functions from the **caret** package.
```{r message=FALSE}
# load caret package
library(caret)
```

We'll use 80% of all the observations for training and the rest for testing.

```{r}
# create train and test datasets
set.seed(10)
train.indices <- createDataPartition(Carseats$HighSales, # the variable defining the class
                                     p = .80,            # the proportion of observations in the training set
                                     list = FALSE)       # do not return the result as a list (which is the default)
train.data <- Carseats[train.indices,]
test.data <- Carseats[-train.indices,]
```

We can check that the distribution of *HighSales* is really (roughly) the same in the two datasets.

```{r}
# print distributions of train and test datasets
prop.table(table(train.data$HighSales))
prop.table(table(test.data$HighSales))
```

# Create a prediction model using Decision Trees

We will use the **rpart** R package to build a decision tree.

Note: this is just one of the available R packages for working with decision trees.

```{r message=FALSE}
# load rpart library
library(rpart)
```

Build a tree using the *rpart* function and all the variables.

```{r}
# build the model
tree1 <- rpart(HighSales ~ ., data = train.data, method = "class")
```

Note the parameter *method*; it is set to the "class" value as we are building a classification tree; if we want to build a regression tree (to perform a regression task), we would set this parameter to 'anova'.

```{r}
# print the model
print(tree1)
```

Let's plot the tree, to understand it better. To that end, we will use the **prp** function from the **rpart.plot** package.

```{r message=FALSE}
# load rpart.plot library
# install.packages("rpart.plot")
library(rpart.plot)
```

Plot the tree.

```{r}
# plot the tree
prp(tree1, type = 3, extra = 1)
```

**TASK**: Check the other options of the type and extra parameters to see how they affect the visualization of the tree model.

Observing the tree, we can see that only a couple of variables were used to build the model: 

* ShelveLo - the quality of the shelving location for the car seats at a given site
* Price - the price the company charges for car seats at a given site
* Advertise - local advertising budget for the company at each location
* Age - the average age of the local population
* CompPrice - price charged by competitors at each location

Let's evaluate our model on the test set.
```{r}
# make the predictions with tree1 over the test dataset
tree1.pred <- predict(object = tree1, newdata = test.data, type = "class")
```

Examine what the predictions look like.

```{r}
# print several predictions
head(tree1.pred)
```

To start evaluating the prediction quality of our model, we will first create the confusion matrix. The **confusion matrix** is used for visualizing and calculating the performance of a classification model. Each row of the matrix represents the instances in an actual class, while each column represents the instances in a predicted class (or vice versa).

```{r knitr-logo, fig.align='center', echo=FALSE, out.width = "400px"}
knitr::include_graphics('images/confusion-matrix.png')
```

In our example, this is the confusion matrix:

```{r}
# create the confusion matrix
tree1.cm <- table(true=test.data$HighSales, predicted=tree1.pred)
tree1.cm
```

There are several measures used for used for evaluating the performance of a classification model.

**Accuracy** is defined as the percentage of correct predictions. Informally, accuracy is the fraction of predictions our model got right. The formula is:

$$Accuracy = \frac{True~Positive + True~Negative}{N}$$

, where N is the total number of predictions.

**Precision** tells us how precise our model is, that is out of observations that are predicted positive, how many of them are actually positive. The formula is: 

$$Precision = \frac{True~Positive}{True~Positive + False~Positive}$$

Precision is a good measure to use in a model where the costs of False Positive is high. For instance, in email spam detection, a False Positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.

**Recall** calculates how many of the actual positives our model capture through labeling it as positive (True Positive). The formula is: 

$$Recall = \frac{True~Positive}{True~Positive + False~Negative}$$

Recall is important in a model when there is a high cost associated with False Negatives. For instance, in a sick patient detection problem, if a sick patient (actual positive) goes through the test and predicted as not sick (predicted negative). The cost associated with False Negative will be extremely high if the sickness is contagious.

**F1-measure** conveys the balance between the Precision and the Recall. The formula is: 

$$F1 = 2 * \frac{Precision * Recall}{Precision + Recall}$$

Since we'll need to compute evaluation metrics couple of times, it's handy to have a function for that. The f. receives a confusion matrix, and returns a named vector with the values for Accuracy, Precision, Recall, and F1-measure.

```{r}
# function for computing evaluation metrix
compute.eval.metrics <- function(cmatrix) {
  TP <- cmatrix[1,1] # true positive
  TN <- cmatrix[2,2] # true negative
  FP <- cmatrix[2,1] # false positive
  FN <- cmatrix[1,2] # false negative
  acc <- sum(diag(cmatrix)) / sum(cmatrix)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2*precision*recall / (precision + recall)
  c(accuracy = acc, precision = precision, recall = recall, F1 = F1)
}
```

Now, we'll use the function to compute evaluation metrics for our tree model.

```{r}
# compute the evaluation metrics
tree1.eval <- compute.eval.metrics(tree1.cm)
tree1.eval
```

Not bad...

The *rpart* function uses a number of parameters to control the growth of a tree. In the above call of the *rpart* function, we relied on the default values of those parameters. To inspect the parameters and their defaults, type:

```{r}
# get the docs for the rpart.control function
?rpart.control
```

Let's now change some of these parameters to try to create a better model. Two parameters that are often considered important are:

- **cp** - the so-called *complexity parameter*. It regulates the splitting of nodes and growing of a tree by preventing splits that are deemed not important enough. In particular, those would be the splits that would not improve the fitness of the model by at least the *cp* value,
- **minsplit** - minimum number of instances in a node for a split to be attempted at that node.

We will decrease the values of both parameters to grow a larger tree.

```{r}
# build the second model with minsplit = 10 and cp = 0.001
tree2 <- rpart(HighSales ~ ., data = train.data, method = "class",
                     control = rpart.control(minsplit = 10, cp = 0.001))

# print the model
print(tree2)
```

Obviously, we got a significantly larger tree. Let's plot this tree.

```{r}
# plot the tree2
prp(tree2, type = 3, extra = 1)
```

Is this larger tree better than the initial one (tree1)? To check that, we need to evaluate the 2nd tree on the test set.

```{r}
# make the predictions with tree2 over the test dataset
tree2.pred <- predict(tree2, newdata = test.data, type = "class")
```

Again, we'll create a confusion matrix.

```{r}
# create the confusion matrix for tree2 predictions
tree2.cm <- table(true=test.data$HighSales, predicted=tree2.pred)
tree2.cm
```

Next, we'll compute the evaluation metrics.

```{r}
# compute the evaluation metrics
tree2.eval <- compute.eval.metrics(tree2.cm)
tree2.eval
```

Let's compare this model to the first one:
```{r}
# compare the evaluation metrics for tree1 and tree2
data.frame(rbind(tree1.eval, tree2.eval), 
           row.names = c("tree 1", "tree 2"))
```

Some metrics have improved, others became worse. So, our guess for the parameter values was not the best one.

Instead of relying on guessing, we should adopt a systematic way of examining the parameters values, looking for the optimal ones. An often applied approach is to cross-validate the model with a range of different values of parameters of interest.

**Cross-validation** is a model validation technique for assessing how the results of statistical analysis (in our case classification) will generalize to an independent data set. The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it. It involves partitioning a dataset into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). In *10-fold cross-validation*, the original dataset is randomly partitioned into 10 equal sized subsamples.

We will apply that approach here, testing the value of the *cp* parameter since it is considered the most important parameter when growing trees with the *rpart* function.

For finding the optimal *cp* value through cross-validation, we will use some handy functions from the **caret** package (it has already been loaded). Since these functions internally call cross-validation functions from the **e1071** package, we need to (install and) load that package.

```{r message=FALSE}
# load e1071 library
# install.packages('e1071')
library(e1071)
```

```{r}
# define cross-validation (cv) parameters; we'll perform 10-fold cross-validation
numFolds = trainControl( method = "cv", number = 10 )

# define the range for the cp values to examine in the cross-validation
cpGrid = expand.grid( .cp = seq(0.001, to = 0.05, by = 0.001)) 
```

Perform parameter search through cross-validation.

```{r}
# since cross-validation is a probabilistic process, it is advisable to set the seed so that we can replicate the results
set.seed(10)

# run the cross-validation
dt.cv <- train(HighSales ~ ., 
               data = train.data, 
               method = "rpart", 
               control = rpart.control(minsplit = 10), 
               trControl = numFolds, 
               tuneGrid = cpGrid)
dt.cv
```

Plot the results of parameter tuning.

```{r}
# plot the cross-validation results
plot(dt.cv)
```

So, we got the best value for the *cp* parameter: 0.041. Since it suggests a simpler model (smaller tree) than the previous one (tree2), we can **prune** the second tree using this *cp* value.

```{r}
# prune the tree2 using the cp = 0.041
tree3 <- prune(tree2, cp = 0.041)

# print the new tree
print(tree3)
```

Create predictions for the *tree3*.

```{r}
# make the predictions with tree3 over the test dataset
tree3.pred <- predict(tree3, newdata = test.data, type = "class")

# create the confusion matrix for tree2 predictions
tree3.cm <- table(true = test.data$HighSales, predicted = tree3.pred)
tree3.cm
```

Compute evaluation metrics for the *tree3*.

```{r}
# compute the evaluation metrics
tree3.eval <- compute.eval.metrics(tree3.cm)
tree3.eval
```

Let's compare all 3 models we have built so far.

```{r}
# compare the evaluation metrics for tree1, tree2 and tree3
data.frame(rbind(tree1.eval, tree2.eval, tree3.eval),
           row.names = c(paste("tree", 1:3)))
```

The 2nd and the 3rd model have the same accuracy but differ in terms of precision and recall. To look for a better model, we might consider altering some other parameters. Another option is to reduce the number of variables that are used for model building.

**TASK**: Create a new tree (tree4) by using only variables that proved relevant in the previous models (tree1, tree2, tree3). Evaluate the model on the test set and compare the evaluation metrics with those obtained for the previous three models.