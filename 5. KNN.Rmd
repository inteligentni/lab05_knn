---
title: "K Nearest Neighbours (KNN)"
output:
  pdf_document: default
  word_document: default
urlcolor: blue
---

# Short Intro to KNN

KNN (K Nearest Neighbours) is a non-parametric, lazy learning algorithm. Non-parametric means it does not make any assumptions on the underlying data distribution. Therefore, KNN should be one of the first choices for a prediction (classification or regression) problem when there is little or no prior knowledge about the data distribution. Lazy algorithm (as opposed to an eager algorithm) means it does not use the training data points (observations) to do any generalization. In other words, there is no explicit training phase.

In KNN, the value of the outcome (response) variable for a given observation is determined based on the outcome values of the nearest *k* neighbors. In order to find the closest neighbors, we calculate either similarity or distance (Euclidean, Manhattan, etc.) between pairs of observations, where each observation is represented as a vector of its feature (attribute) values.

When KNN is used for classification, a new observation is classified by taking a majority vote of its nearest neighbors. In other words, the observation is assigned to the class that is the most common among its k nearest neighbors. KNN can also be used for regression. In that case, the outcome, that is, the predicted continuous value, is calculated as the average (mean or median) of the outcome values of its k nearest neighbors.

The steps are the following:

1. Calculate the distance between a new observation (the one for which a prediction is to be made) and each observation from the training set,
2. For the new observation, find *k* closest observations in the training set (= k nearest neighbors),
3. In case of classification, assign label (class) to the new observation based on the majority class of its nearest neighbors; in case of regression, compute the predicted outcome value as the average value of the outcome variable of the nearest neighbors.   

The following figures illustrate the classification steps.

```{r knitr-logo, fig.align='center', out.height="390px", echo=FALSE}
knitr::include_graphics(rep('images/knn-steps.png'))
```


# Load and prepare data set

We will use the same data set (*Carseats*) as in the previous Lab.

```{r}
# load ISLR package
library(ISLR)

# print dataset structure
str(Carseats)
```

As we did before, we will introduce a categorical (factor) variable *HighSales* to be used as the outcome variable (variable defining the class for each observation). If a sale is greater than the 3rd quartile, it qualifies as a high sale.

```{r}
# calculate 3rd quartile
sales.3Q <- quantile(Carseats$Sales, 0.75)

# create a new variable HighSales based on the value of the 3rd quartile
Carseats$HighSales <- ifelse(test = Carseats$Sales > sales.3Q,
                             yes = 'Yes',
                             no = 'No')

# convert HighSales from character to factor
Carseats$HighSales <- as.factor(Carseats$HighSales)
```

We'll remove the *Sales* variable - since it was used for the creation of the outcome variable, it should not be used for prediction.

```{r}
# remove the Sales variable
Carseats <- Carseats[,-1]
```

# Standardize numerical attributes

Recall that the kNN algorithm works with numerical data. So, if we want to use categorical and/or binary variables, we have to transform them into numerical variables. Note: this transformation should be done for categorical variables *if and only if* it makes sense to represent a categorical variable as a numerical value. For example, transforming race or ethnicity to a numerical variable does not make sense (what would it mean to add or subtract Asian from Caucasian race?), and thus such variables, if present in the dataset, should *not* be used for kNN. 

kNN is very sensitive to differences in the value range of predictor variables. This is because predictors with a wider value range (e.g. *Price*) would diminish the influence of variables with significantly narrower range (e.g. *Education*). 

Let's check our variables and their value ranges.

```{r}
# print the summary of the dataset
summary(Carseats)
```

Value intervals differ for all variables. We should, obviously, rescale our numerical variables. 

Rescaling can be generally done in two ways:

* *Normalization* - reducing variable values to a common value range, typically [0,1]; this is often done using the formula: 

$$Z = \frac{X - min(X)}{max(X) - min(X)}$$

* *Standardization* - rescaling variables so that their *mean = 0* and *SD = 1*. For the variable X that is normally distributed, this is done by computing: 

$$Z = \frac{X - mean(X)}{SD(X)}$$

\begin{itemize}
  \item[] If the variable X is not normally distributed, standardization is typically done using median and inter-quartile range (IQR):
\end{itemize}

$$Z = \frac{X - median(X)}{IQR(X)}$$
\begin{itemize}
  \item[] , where $IQR(X) = Q3(X) - Q1(X)$.
\end{itemize}

Normalization should be avoided if (numerical) variables have outliers; standardization should be used instead. In the absence of outliers, either of the two can be used.

Let's check the presence of outliers in the *CompPrice* variable.

```{r}
# plot the boxplot for the CompPrice variable
boxplot(Carseats$CompPrice)

# print the number of outliers in the CompPrice variable
length(boxplot.stats(Carseats$CompPrice)$out)
```

Let's do the same for all numeric variables.

```{r}
# select numeric variables
numeric.vars <- c(1:5,7,8) # indices of numeric columns

# apply the function that returns the number of outliers to each numeric column
apply(X = Carseats[,numeric.vars], # select numeric columns
      MARGIN = 2, # apply the function to columns
      FUN = function(x) length(boxplot.stats(x)$out)) # the function to be applied to each column 
```

Two variables (*CompPrice*, *Price*) have a few outliers. Hence, it is better to use standardization to rescale the variables.

To determine how to standardize numeric variables, we need to check their distribution, that is, if they follow Normal distribution or not. 

We will use *Shapiro-Wilk test* to check for normality. The *null hypothesis* of this test is that a sample comes from a normally distributed population; if the test is not significant (p>0.05), we can assume that the null hypothesis holds and the variable is normally distributed.

```{r}
# apply the Shapiro-Wilk test to each numerical column (variable)
apply(X = Carseats[,numeric.vars], 
      MARGIN = 2, 
      FUN = shapiro.test)
```

Only *CompPrice* and *Price* are normally distributed. So, we will standardize *Price* and *CompPrice* using mean and SD, and for the other variables, we'll use median and IQR. 

To do the scaling, we will use the *scale* function (from the base package).

```{r}
# get the documentation for the scale function
?scale
```

We'll start by rescaling variables that are not normally distributed.

```{r}
# select not-normally distributed numerical columns (variables)
carseats.st <- Carseats[, c(2,3,4,7,8)]

# apply the scalling function to each column
carseats.st <- apply(X = carseats.st, 
                     MARGIN = 2,
                     FUN = function(x) scale(x, center = median(x), scale = IQR(x)))

# since apply() f. returns a list, convert it to a data frame
carseats.st <- as.data.frame(carseats.st)
```

Then, we'll standardize and add normally distributed ones.

```{r}
# standardize the Price variable (and convert to vector)
carseats.st$Price <- as.vector(scale(x = Carseats$Price, center = TRUE, scale = TRUE))

# standardize the CompPrice variable (and convert to vector)
carseats.st$CompPrice <- as.vector(scale(x = Carseats$CompPrice, center = TRUE, scale = TRUE))
```

**Note:** the scale() f. returns a matrix with just one column; so, it is effectively a vector and we transform it into a vector using the as.vector() f.

Now, we need to handle binary and categorical variables.

# Transform factor (binary and categorical) variables

Transform binary variables into numerical.

```{r}
# transform the Urban variable to integer
carseats.st$Urban <- as.integer(Carseats$Urban)

# transform the US variable to integer
carseats.st$US <- as.integer(Carseats$US)
```

We have one categorical (non-binary) variable: *ShelveLoc*. Since this variable is ordered (with levels "Bad", "Medium", "Good"),
transformation to numerical values is acceptable. 

First, let's check the order of *ShelveLoc* levels.
```{r}
# print the levels of the ShelveLoc variable
levels(Carseats$ShelveLoc)
```

Obviously, the order is not a 'natural' one. So, we need to change the order of levels.

```{r}
# update the order of levels for the ShelveLoc variable to: "Bad", "Medium", "Good"
Carseats$ShelveLoc <- factor(Carseats$ShelveLoc, levels = c("Bad", "Medium", "Good"))
levels(Carseats$ShelveLoc)
```

Now, we can transform the *ShelveLoc* into a numerical variable.

```{r}
# convert ShelveLoc into a numeric variable
carseats.st$ShelveLoc <- as.integer(Carseats$ShelveLoc)
```

**TASK**: It is often considered more correct to first encode categorical variables as binary dummy variables, and then transform the resulting binary variables into numerical ones. However, for simplicity reasons, we applied direct transformation.
Try to create dummy variables for *ShelveLoc* and build a model with these new variables; [this page](http://topepo.github.io/caret/pre-processing.html#dummy) shows how to create dummy variables using the *caret* package.


Finally, add the outcome (class) variable to the transformed dataset.

```{r}
# add the outcome variable HighSales
carseats.st$HighSales <- Carseats$HighSales
```

Examine the transformed data set.

```{r}
# examine the structure of the transformed dataset
str(carseats.st)
```

```{r}
# examine the summary of the transformed dataset
summary(carseats.st)
```


Now that we have prepared the data, we can proceed to create sets for training and testing.

# Create train and test data sets

We'll use the *caret* package for partitioning the dataset into train and test sets.

```{r message=FALSE}
# load the caret package
library(caret)
```

We'll take 80% of observations for the training set and the rest for the test set.

```{r}
# set seed
set.seed(10320)

# create train and test sets
train.indices <- createDataPartition(carseats.st$HighSales, p = 0.8, list = FALSE)
train.data <- carseats.st[train.indices,]
test.data <- carseats.st[-train.indices,]
```

# Model building

To build a kNN classification model, we will use the *knn* f. from the *class* package.

```{r}
# load the class package
library(class)

?knn
```

As the knn() function description indicates, we need to provide the function with:

* training data without the class variable,
* test data without the class variable,
* class values for the training set,
* the number of neighbors to consider.

```{r}
# create a knn model with k=5
knn.pred <- knn(train = train.data[,-11], 
                test = test.data[,-11],
                cl = train.data$HighSales,
                k = 5) # 5 is chosen here just as a random guess
```

The result of the *knn* f. are, in fact, predictions on the test set.

```{r}
# print several predictions
head(knn.pred)
```

To evaluate the results, we'll first create the confusion matrix:

```{r}
# create the confusion matrix
knn.cm <- table(true = test.data$HighSales, predicted = knn.pred)
knn.cm
```

We'll use the function for computing the evaluation metrics. 
Recall (from the previous Lab) that we set No as the positive class.

```{r}
# function for computing evaluation metrics
compute.eval.metrics <- function(cmatrix) {
  TP <- cmatrix[1,1] # true positive
  TN <- cmatrix[2,2] # true negative
  FP <- cmatrix[2,1] # false positive
  FN <- cmatrix[1,2] # false negative
  acc = sum(diag(cmatrix)) / sum(cmatrix)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2*precision*recall / (precision + recall)
  c(accuracy = acc, precision = precision, recall = recall, F1 = F1)
}
```

Compute the evaluation metrics based on the confusion matrix.

```{r}
# compute the evaluation metrics
knn.eval <- compute.eval.metrics(knn.cm)
knn.eval
```

Not bad, but we might be able to do better by choosing value for k in a more systematic way.

We made a guess about the number of neighbors, and might not have made the best guess. Instead of guessing, we'll cross-validate models with several different values for k, and see which one gives the best performance; then, we'll use the test set to evaluate the model that proves to be the best on cross-validation.

For finding the optimal value for *k* through 10-fold cross-validation, we will use the **caret** package and the **e1071** package (internally called by the *caret* package).

```{r message=FALSE}
# load e1071 library
library(e1071)
```

```{r}
# define cross-validation (cv) parameters; we'll perform 10-fold cross-validation
numFolds = trainControl( method = "cv", number = 10)
```

Then, define the range of *k* values to examine in the cross-validation. We'll take odd numbers between 3 and 25. Recall that in case of binary classification, it is recommended to choose an odd number for *k* - this is to avoid ties when deciding on the majority class.

```{r}
# define the range for the k values to examine in the cross-validation
cpGrid = expand.grid(.k = seq(from=3, to = 25, by = 2))
```

Now, train the model through cross-validation.

```{r}
# since cross-validation is a probabilistic process, it is advisable to set the seed so that we can replicate the results
set.seed(10320)

# run the cross-validation
knn.cv <- train(x = train.data[,-11],
                y = train.data$HighSales,
                method = "knn", 
                trControl = numFolds,
                tuneGrid = cpGrid)
knn.cv
```

We can get a better insight into the cross-validation results by plotting them.

```{r}
# plot the cross-validation results
plot(knn.cv)
```

k=`r knn.cv$bestTune$k` proved to be the best value. Let's build a model with that value for k.

```{r}
# build a new model with the best value for k
best_k <- knn.cv$bestTune$k
knn.pred2 <- knn(train = train.data[,-11],
                  test = test.data[,-11],
                  cl = train.data$HighSales,
                  k = best_k)
```

Create the confusion matrix for the new predictions.

```{r}
# create the confusion matrix
knn.cm2 <- table(true = test.data$HighSales, predicted = knn.pred2)
knn.cm2
```

Compute evaluation measures.

```{r}
# compute the evaluation metrics
knn.eval2 <- compute.eval.metrics(knn.cm2)
knn.eval2
```

This model seems to be better than the previous one, but let's compare the metrics of the two models to check how the new model fares
```{r}
# compare the evaluation metrics for knn1 and knn2 models
data.frame(rbind(knn.eval, knn.eval2), row.names = c("knn_1", "knn_2"))
```

The second model (*knn_2*) is better with respect to all metrics.

**TASK** Create a new model by taking only a subset of variables, for example, those that proved relevant in the DT model and compare the performance with the previously built models.

Potentially useful articles:

* [An excellent article on how the selection of *k* affects the model's complexity and ability to generalise](https://cambridgecoding.wordpress.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/)
* [kNN Using caret R package](http://rpubs.com/njvijay/16444)
* [Knn classifier implementation in R with caret package](http://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/)